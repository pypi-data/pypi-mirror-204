{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing a Gensim model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how to use [`pyLDAvis`](https://github.com/bmabey/pyLDAvis)'s gensim [helper funtions](https://pyldavis.readthedocs.org/en/latest/modules/API.html#module-pyLDAvis.gensim) we will create a model from the [20 Newsgroup corpus](http://qwone.com/~jason/20Newsgroups/). Minimal preprocessing is done and so the model is not the best. However, the goal of this notebook is to demonstrate the helper functions.\n",
    "\n",
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/DATASCIENCE/github/pyLDAvis/notebooks/data ~/DATASCIENCE/github/pyLDAvis/notebooks\n",
      "The data has already been downloaded...\n",
      "Lets take a look at the groups...\n",
      "\u001b[34malt.atheism\u001b[m\u001b[m\n",
      "\u001b[34mcomp.graphics\u001b[m\u001b[m\n",
      "\u001b[34mcomp.os.ms-windows.misc\u001b[m\u001b[m\n",
      "\u001b[34mcomp.sys.ibm.pc.hardware\u001b[m\u001b[m\n",
      "\u001b[34mcomp.sys.mac.hardware\u001b[m\u001b[m\n",
      "\u001b[34mcomp.windows.x\u001b[m\u001b[m\n",
      "\u001b[34mmisc.forsale\u001b[m\u001b[m\n",
      "\u001b[34mrec.autos\u001b[m\u001b[m\n",
      "\u001b[34mrec.motorcycles\u001b[m\u001b[m\n",
      "\u001b[34mrec.sport.baseball\u001b[m\u001b[m\n",
      "\u001b[34mrec.sport.hockey\u001b[m\u001b[m\n",
      "\u001b[34msci.crypt\u001b[m\u001b[m\n",
      "\u001b[34msci.electronics\u001b[m\u001b[m\n",
      "\u001b[34msci.med\u001b[m\u001b[m\n",
      "\u001b[34msci.space\u001b[m\u001b[m\n",
      "\u001b[34msoc.religion.christian\u001b[m\u001b[m\n",
      "\u001b[34mtalk.politics.guns\u001b[m\u001b[m\n",
      "\u001b[34mtalk.politics.mideast\u001b[m\u001b[m\n",
      "\u001b[34mtalk.politics.misc\u001b[m\u001b[m\n",
      "\u001b[34mtalk.religion.misc\u001b[m\u001b[m\n",
      "~/DATASCIENCE/github/pyLDAvis/notebooks\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir -p data\n",
    "pushd data\n",
    "if [ -d \"20news-bydate-train\" ]\n",
    "then\n",
    "  echo \"The data has already been downloaded...\"\n",
    "else\n",
    "  wget http://qwone.com/%7Ejason/20Newsgroups/20news-bydate.tar.gz\n",
    "  tar xfv 20news-bydate.tar.gz\n",
    "  rm 20news-bydate.tar.gz\n",
    "fi\n",
    "echo \"Lets take a look at the groups...\"\n",
    "ls 20news-bydate-train/\n",
    "popd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each group dir has a set of files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--@   1 msusol  primarygroup   1.4K Mar 18  2003 61250\r\n",
      "-rw-r--r--@   1 msusol  primarygroup   889B Mar 18  2003 61252\r\n",
      "-rw-r--r--@   1 msusol  primarygroup   1.2K Mar 18  2003 61264\r\n",
      "-rw-r--r--@   1 msusol  primarygroup   1.6K Mar 18  2003 61308\r\n",
      "-rw-r--r--@   1 msusol  primarygroup   1.3K Mar 18  2003 61422\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah data/20news-bydate-train/sci.space | tail  -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a peak at one email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: ralph.buttigieg@f635.n713.z3.fido.zeta.org.au (Ralph Buttigieg)\r\n",
      "Subject: Why not give $1 billion to first year-lo\r\n",
      "Organization: Fidonet. Gate admin is fido@socs.uts.edu.au\r\n",
      "Lines: 34\r\n",
      "\r\n",
      "Original to: keithley@apple.com\r\n",
      "G'day keithley@apple.com\r\n",
      "\r\n",
      "21 Apr 93 22:25, keithley@apple.com wrote to All:\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head data/20news-bydate-train/sci.space/61422"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and tokenizing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "import string\n",
    "import funcy as fp\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">talk.politics.mideast</th>\n",
       "      <th>75895</th>\n",
       "      <td>[From: hm@cs.brown.edu (Harry Mamaysky)\\n, Sub...</td>\n",
       "      <td>[from, #email, harry, mamaysky, subject, heil,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76248</th>\n",
       "      <td>[From: waldo@cybernet.cse.fau.edu (Todd J. Dic...</td>\n",
       "      <td>[from, #email, todd, dicker, subject, israel's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76277</th>\n",
       "      <td>[From: C.L.Gannon@newcastle.ac.uk (Space Cadet...</td>\n",
       "      <td>[from, #email, space, cadet, subject, exact, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76045</th>\n",
       "      <td>[From: shaig@Think.COM (Shai Guday)\\n, Subject...</td>\n",
       "      <td>[from, #email, shai, guday, subject, basil, op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76283</th>\n",
       "      <td>[From: koc@rize.ECE.ORST.EDU (Cetin Kaya Koc)\\...</td>\n",
       "      <td>[from, #email, cetin, kaya, koc, subject, seve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           doc   \n",
       "group                 id                                                         \n",
       "talk.politics.mideast 75895  [From: hm@cs.brown.edu (Harry Mamaysky)\\n, Sub...  \\\n",
       "                      76248  [From: waldo@cybernet.cse.fau.edu (Todd J. Dic...   \n",
       "                      76277  [From: C.L.Gannon@newcastle.ac.uk (Space Cadet...   \n",
       "                      76045  [From: shaig@Think.COM (Shai Guday)\\n, Subject...   \n",
       "                      76283  [From: koc@rize.ECE.ORST.EDU (Cetin Kaya Koc)\\...   \n",
       "\n",
       "                                                                        tokens  \n",
       "group                 id                                                        \n",
       "talk.politics.mideast 75895  [from, #email, harry, mamaysky, subject, heil,...  \n",
       "                      76248  [from, #email, todd, dicker, subject, israel's...  \n",
       "                      76277  [from, #email, space, cadet, subject, exact, m...  \n",
       "                      76045  [from, #email, shai, guday, subject, basil, op...  \n",
       "                      76283  [from, #email, cetin, kaya, koc, subject, seve...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick and dirty....\n",
    "EMAIL_REGEX = re.compile(r\"[a-z0-9\\.\\+_-]+@[a-z0-9\\._-]+\\.[a-z]*\")\n",
    "FILTER_REGEX = re.compile(r\"[^a-z '#]\")\n",
    "TOKEN_MAPPINGS = [(EMAIL_REGEX, \"#email\"), (FILTER_REGEX, ' ')]\n",
    "\n",
    "def tokenize_line(line):\n",
    "    res = line.lower()\n",
    "    for regexp, replacement in TOKEN_MAPPINGS:\n",
    "        res = regexp.sub(replacement, res)\n",
    "    return res.split()\n",
    "    \n",
    "def tokenize(lines, token_size_filter=2):\n",
    "    tokens = fp.mapcat(tokenize_line, lines)\n",
    "    return [t for t in tokens if len(t) > token_size_filter]\n",
    "    \n",
    "\n",
    "def load_doc(filename):\n",
    "    group, doc_id = filename.split('/')[-2:]\n",
    "    with open(filename, errors='ignore') as f:\n",
    "        doc = f.readlines()\n",
    "    return {'group': group,\n",
    "            'doc': doc,\n",
    "            'tokens': tokenize(doc),\n",
    "            'id': doc_id}\n",
    "\n",
    "\n",
    "docs = pd.DataFrame(list(map(load_doc, glob('data/20news-bydate-train/*/*')))).set_index(['group','id'])\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dictionary, and bag of words corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nltk_stopwords():\n",
    "    return set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prep_corpus(docs, additional_stopwords=set(), no_below=5, no_above=0.5):\n",
    "  print('Building dictionary...')\n",
    "  dictionary = Dictionary(docs)\n",
    "  stopwords = nltk_stopwords().union(additional_stopwords)\n",
    "  stopword_ids = map(dictionary.token2id.get, stopwords)\n",
    "  dictionary.filter_tokens(stopword_ids)\n",
    "  dictionary.compactify()\n",
    "  dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=None)\n",
    "  dictionary.compactify()\n",
    "\n",
    "  print('Building corpus...')\n",
    "  corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "  return dictionary, corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resource stopwords not found.\n",
    "Please use the NLTK Downloader to obtain the resource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "Building corpus...\n"
     ]
    }
   ],
   "source": [
    "dictionary, corpus = prep_corpus(docs['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MmCorpus.serialize('newsgroups.mm', corpus)\n",
    "dictionary.save('newsgroups.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 53s, sys: 40.6 s, total: 6min 33s\n",
      "Wall time: 40.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=50, passes=10)\n",
    "                                      \n",
    "lda.save('newsgroups_50_lda.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the model with pyLDAvis\n",
    "\n",
    "Okay, the moment we have all been waiting for is finally here!  You'll notice in the visualization that we have a few junk topics that would probably disappear after better preprocessing of the corpus. This is left as an exercises to the reader. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = gensimvis.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the HDP model\n",
    "\n",
    "We can both visualize LDA models as well as gensim HDP models with pyLDAvis.\n",
    "\n",
    "The difference between HDP and LDA is that HDP is a non-parametric method. Which means that we don't need to specify the number of topics. HDP will fit as many topics as it can and find the optimal number of topics by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.6 s, sys: 58.6 s, total: 1min 46s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The optional parameter T here indicates that HDP should find no more than 50 topics\n",
    "# if there exists any.\n",
    "hdp = models.hdpmodel.HdpModel(corpus, dictionary, T=50)\n",
    "                                      \n",
    "hdp.save('newsgroups_hdp.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the HDP model with pyLDAvis\n",
    "\n",
    "As for the LDA model, in order to prepare the visualization you only need to pass it your model, the corpus, and the associated dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = gensimvis.prepare(hdp, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/msusol/DATASCIENCE/github/pyLDAvis\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyLDAvis==3.4.1) (1.24.2)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyLDAvis==3.4.1) (1.10.1)\n",
      "Requirement already satisfied: pandas>=1.3.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyLDAvis==3.4.1) (2.0.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyLDAvis==3.4.1) (1.2.0)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyLDAvis==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: numexpr in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyLDAvis==3.4.1) (2.8.4)\n",
      "Requirement already satisfied: funcy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyLDAvis==3.4.1) (2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyLDAvis==3.4.1) (1.2.2)\n",
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyLDAvis==3.4.1) (4.3.1)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyLDAvis==3.4.1) (65.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas>=1.3.4->pyLDAvis==3.4.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas>=1.3.4->pyLDAvis==3.4.1) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas>=1.3.4->pyLDAvis==3.4.1) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn>=1.0.0->pyLDAvis==3.4.1) (3.1.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim->pyLDAvis==3.4.1) (6.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->pyLDAvis==3.4.1) (2.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.4->pyLDAvis==3.4.1) (1.16.0)\n",
      "Building wheels for collected packages: pyLDAvis\n",
      "  Building wheel for pyLDAvis (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyLDAvis: filename=pyLDAvis-3.4.1-py3-none-any.whl size=2600891 sha256=0b611ea12c1a31f4c54e7d4247958ab996234a0d0a70a809452b6a9e640915fc\n",
      "  Stored in directory: /private/var/folders/xt/6scrt9hx2dv4q54bvv003d28006lxt/T/pip-ephem-wheel-cache-ux5ncvwv/wheels/80/9b/83/7a3bb1f5732da340c618bb32c8cafda5991ca15f006629e954\n",
      "Successfully built pyLDAvis\n",
      "Installing collected packages: pyLDAvis\n",
      "  Attempting uninstall: pyLDAvis\n",
      "    Found existing installation: pyLDAvis 3.4.1\n",
      "    Uninstalling pyLDAvis-3.4.1:\n",
      "      Successfully uninstalled pyLDAvis-3.4.1\n",
      "Successfully installed pyLDAvis-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ~/DATASCIENCE/github/pyLDAvis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyLDAvis\n",
      "Version: 3.4.1\n",
      "Summary: Interactive topic model visualization. Port of the R package.\n",
      "Home-page: https://github.com/bmabey/pyLDAvis\n",
      "Author: Ben Mabey\n",
      "Author-email: ben@benmabey.com\n",
      "License: BSD-3-Clause\n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages\n",
      "Requires: funcy, gensim, jinja2, joblib, numexpr, numpy, pandas, scikit-learn, scipy, setuptools\n",
      "Required-by: \n",
      "Name: pandas\n",
      "Version: 2.0.0\n",
      "Summary: Powerful data structures for data analysis, time series, and statistics\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: The Pandas Development Team <pandas-dev@python.org>\n",
      "License: BSD 3-Clause License\n",
      "\n",
      "Copyright (c) 2008-2011, AQR Capital Management, LLC, Lambda Foundry, Inc. and PyData Development Team\n",
      "All rights reserved.\n",
      "\n",
      "Copyright (c) 2011-2023, Open source contributors.\n",
      "\n",
      "Redistribution and use in source and binary forms, with or without\n",
      "modification, are permitted provided that the following conditions are met:\n",
      "\n",
      "* Redistributions of source code must retain the above copyright notice, this\n",
      "  list of conditions and the following disclaimer.\n",
      "\n",
      "* Redistributions in binary form must reproduce the above copyright notice,\n",
      "  this list of conditions and the following disclaimer in the documentation\n",
      "  and/or other materials provided with the distribution.\n",
      "\n",
      "* Neither the name of the copyright holder nor the names of its\n",
      "  contributors may be used to endorse or promote products derived from\n",
      "  this software without specific prior written permission.\n",
      "\n",
      "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "\n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages\n",
      "Requires: numpy, numpy, python-dateutil, pytz, tzdata\n",
      "Required-by: pyLDAvis\n"
     ]
    }
   ],
   "source": [
    "!pip show pyLDAvis\n",
    "!pip show pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving to http://127.0.0.1:8888/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [23/Apr/2023 16:06:22] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.show(vis, local=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.disable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.show(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "name": "Gensim Newsgroup.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
