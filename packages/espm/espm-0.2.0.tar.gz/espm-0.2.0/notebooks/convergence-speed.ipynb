{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from snmfem.experiments import load_samples, print_results, load_data, run_experiment\n",
    "from espm.measures import KL, trace_xtLx\n",
    "from espm.estimators.updates import multiplicative_step_h, multiplicative_step_w, multiplicative_step_hq\n",
    "from espm.conf import log_shift, dicotomy_tol, sigmaL\n",
    "from espm.utils import create_laplacian_matrix\n",
    "from espm.estimators.smooth_nmf import diff_surrogate\n",
    "from espm.estimators import SmoothNMF\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_toy_problem(l = 25, k = 3, p = 100, c = 10, n_poisson=200, force_simplex=True):\n",
    "\n",
    "    A = np.random.rand(k,p)\n",
    "    if force_simplex:\n",
    "        A = A/np.sum(A, axis=0, keepdims=True)\n",
    "    \n",
    "    G = np.random.rand(l,c)\n",
    "    P = np.random.rand(c,k)\n",
    "    GP = G @ P\n",
    "\n",
    "    X = GP @ A\n",
    "\n",
    "    Xdot = 1/n_poisson * np.random.poisson(n_poisson * X)\n",
    "\n",
    "    return G, P, A, X, Xdot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_experiment(X, W0, H0, experiment_param, algo_param, global_param):\n",
    "    est = SmoothNMF(**algo_param, **experiment_param, **global_param)\n",
    "    W = est.fit_transform(X, W=W0, H=H0)\n",
    "    H = est.H_\n",
    "    losses = est.get_losses()\n",
    "    loss = losses[\"full_loss\"].copy()\n",
    "    final_loss = loss[-1]\n",
    "    gamma = losses[\"gamma\"].copy()\n",
    "    return loss, final_loss, W.copy(), H.copy(), gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "global_param = dict()\n",
    "global_param[\"l2\"] = False\n",
    "global_param[\"verbose\"]= 0\n",
    "global_param[\"tol\"] = 0\n",
    "global_param[\"max_iter\"] = 1000\n",
    "global_param[\"dicotomy_tol\"] = dicotomy_tol\n",
    "global_param[\"debug\"] = False\n",
    "global_param[\"log_shift\"] = log_shift\n",
    "global_param[\"eval_print\"] = 10\n",
    "global_param[\"hspy_comp\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shape_2d = [10, 15]\n",
    "k = 5\n",
    "n_poisson = 200\n",
    "G, P, A, Xtrue, X = create_toy_problem(p = shape_2d[0]*shape_2d[1], k=k, n_poisson=n_poisson)\n",
    "\n",
    "true_D = G @ P\n",
    "true_H = A\n",
    "\n",
    "# X = Xtrue\n",
    "L = create_laplacian_matrix(*shape_2d)\n",
    "lambda_L = 20\n",
    "\n",
    "W0 = np.random.rand(*true_D.shape)\n",
    "H0 = np.random.rand(*A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# experiment parameters\n",
    "experiment_param = dict()\n",
    "experiment_param[\"force_simplex\"] = False\n",
    "experiment_param[\"lambda_L\"] = lambda_L \n",
    "experiment_param[\"mu\"] = 0\n",
    "experiment_param[\"epsilon_reg\"] = 1\n",
    "experiment_param[\"normalize\"] = False\n",
    "experiment_param[\"G\"] = None\n",
    "experiment_param[\"shape_2d\"] = shape_2d\n",
    "experiment_param[\"n_components\"] = k\n",
    "experiment_param[\"true_D\"] = true_D\n",
    "experiment_param[\"true_H\"] = true_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "final_losses = []\n",
    "Ws = []\n",
    "Hs = []\n",
    "params = []\n",
    "captions = []\n",
    "gammas = []\n",
    "for algo in [\"log_surrogate\", \"l2_surrogate\", \"projected_gradient\"]:\n",
    "    for linesearch in [False, True]:\n",
    "        # for sL in [sigmaL/4, sigmaL/2, sigmaL]:\n",
    "        for sL in [sigmaL]:\n",
    "            # algo parameters\n",
    "            algo_param = dict()\n",
    "            algo_param[\"linesearch\"] = linesearch\n",
    "            algo_param[\"algo\"] = algo\n",
    "            # algo_param[\"gamma\"] = sL\n",
    "            if algo == \"projected_gradient\":\n",
    "                algo_param[\"gamma\"] = [500*sL, 500*sL]\n",
    "            else:\n",
    "                algo_param[\"gamma\"] = sL\n",
    "\n",
    "            loss, final_loss, W, H, gamma = one_experiment(X, W0, H0, experiment_param, algo_param, global_param)\n",
    "            losses.append(loss)\n",
    "            final_losses.append(final_loss)\n",
    "            params.append([experiment_param, algo_param, global_param])\n",
    "            Ws.append(W)\n",
    "            Hs.append(H)\n",
    "            gammas.append(gamma)\n",
    "            cl = \"\" if linesearch else \"no\"\n",
    "            captions.append(f\"{algo} - {cl} linesearch - $\\gamma_0$={sL}\")\n",
    "            # captions.append(f\"{algo} - {cl} linesearch\")\n",
    "\n",
    "i = np.argmin(final_losses)\n",
    "global_param_m = deepcopy(global_param)\n",
    "global_param_m[\"max_iter\"] = global_param_m[\"max_iter\"]*3\n",
    "_, l_infty, _, _, _ = one_experiment(X, Ws[i], Hs[i], experiment_param, params[i][1], global_param_m)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 6])\n",
    "# plt.figure(figsize=[15, 10])\n",
    "\n",
    "for loss, caption in zip(losses, captions):\n",
    "    iterations = np.arange(len(loss))+1\n",
    "    if len(iterations)>10:\n",
    "        plt.plot(iterations, loss-l_infty, \".-\", label=caption)\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gamma, caption in zip(gammas, captions):\n",
    "    iterations = np.arange(len(gamma))+1\n",
    "\n",
    "    plt.plot(iterations, gamma, \".\", label=caption)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shape_2d = [10, 15]\n",
    "k = 5\n",
    "n_poisson = 200\n",
    "G, P, A, Xtrue, X = create_toy_problem(p = shape_2d[0]*shape_2d[1], k=k, n_poisson=n_poisson)\n",
    "\n",
    "# X = Xtrue\n",
    "L = create_laplacian_matrix(*shape_2d)\n",
    "lambda_L = 20\n",
    "\n",
    "W1 = np.random.rand(*P.shape)\n",
    "H1 = np.random.rand(*A.shape)\n",
    "# H1 = H1/np.sum(H1, axis=0, keepdims=True)\n",
    "W2 = W1.copy()\n",
    "H2 = H1.copy()\n",
    "\n",
    "W3 = W1.copy()\n",
    "H3 = H1.copy()\n",
    "\n",
    "H4 = H1.copy()\n",
    "W4 = W1.copy()\n",
    "\n",
    "H5 = H1.copy()\n",
    "W5 = W1.copy()\n",
    "\n",
    "W0 = W1.copy()\n",
    "H0 = H1.copy()\n",
    "\n",
    "\n",
    "\n",
    "def loss(P, A):\n",
    "    DA = G @ P @ A\n",
    "    v1 = KL(X, DA) \n",
    "    v2 = trace_xtLx(L, A.T)\n",
    "    return v1 + lambda_L/2 * v2\n",
    "\n",
    "maxit = 100\n",
    "force_simplex = False\n",
    "loss0 = [loss(W0, H0)]\n",
    "\n",
    "loss1 = [loss(W1, H1)]\n",
    "loss2 = [loss(W2, H2)]\n",
    "diff_loss2 = []\n",
    "loss3 = [loss(W3, H3)]\n",
    "diff_loss3 = []\n",
    "\n",
    "loss4 = [loss(W4, H4)]\n",
    "diff_loss4 = []\n",
    "loss5 = [loss(W5, H5)]\n",
    "diff_loss5 = []\n",
    "\n",
    "# gammH2 = sigmaL\n",
    "gammH2 = 1e-3\n",
    "gammH2 = 0.005\n",
    "gammH2 = 0.25\n",
    "\n",
    "gammH5 = 1e-3\n",
    "gammH5 = 0.005\n",
    "gammH5 = 0.4\n",
    "\n",
    "gammH3 = sigmaL\n",
    "# gammH3 = 0.4\n",
    "gammH3_vec = [gammH3]\n",
    "d3 = []\n",
    "\n",
    "gammH4 = sigmaL\n",
    "# gammH4 = 0.25\n",
    "gammH4_vec = [gammH4]\n",
    "d4 = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(maxit):\n",
    "    H0 = multiplicative_step_h(X, G, W0, H0, force_simplex=force_simplex, lambda_L=lambda_L, L=L)\n",
    "    W0 = multiplicative_step_w(X, G, W0, H0)\n",
    "    loss0.append(loss(W0, H0))\n",
    "    \n",
    "    H1 = multiplicative_step_hq(X, G, W1, H1, force_simplex=force_simplex, lambda_L=lambda_L, L=L)\n",
    "    W1 = multiplicative_step_w(X, G, W1, H1)\n",
    "    loss1.append(loss(W1, H1))\n",
    "\n",
    "    \n",
    "    H2 = multiplicative_step_hq(X, G, W2, H2, force_simplex=force_simplex, lambda_L=lambda_L, L=L, sigmaL=gammH2)\n",
    "    diff_loss2.append(loss2[-1]-loss(W2, H2))\n",
    "    W2 = multiplicative_step_w(X, G, W2, H2)\n",
    "    loss2.append(loss(W2, H2))\n",
    "\n",
    "    H3_old = H3.copy()\n",
    "    H3 = multiplicative_step_hq(X, G, W3, H3, force_simplex=force_simplex, lambda_L=lambda_L, L=L, sigmaL=gammH3)\n",
    "    d3.append(diff_surrogate(H3_old, H3, L=L, sigmaL=gammH3))\n",
    "    diff_loss3.append(d3)\n",
    "    if d3[-1]>0:\n",
    "        gammH3 = gammH3 / 1.05\n",
    "    else:\n",
    "        gammH3 = gammH3 * 1.5\n",
    "    gammH3_vec.append(gammH3)\n",
    "    W3 = multiplicative_step_w(X, G, W3, H3)\n",
    "    loss3.append(loss(W3, H3))\n",
    "    \n",
    "    H4_old = H4.copy()\n",
    "    H4 = multiplicative_step_h(X, G, W4, H4, force_simplex=force_simplex, lambda_L=lambda_L, L=L, sigmaL=gammH4)\n",
    "    d4.append(diff_surrogate(H4_old, H4, L=L, sigmaL=gammH4, dgkl=True))\n",
    "    diff_loss4.append(d4)\n",
    "    if d4[-1]>0:\n",
    "        gammH4 = gammH4 / 1.05\n",
    "    else:\n",
    "        gammH4 = gammH4 * 1.5\n",
    "    gammH4_vec.append(gammH4)\n",
    "    W4 = multiplicative_step_w(X, G, W4, H4)\n",
    "    loss4.append(loss(W4, H4))\n",
    "    \n",
    "    H5 = multiplicative_step_hq(X, G, W5, H5, force_simplex=force_simplex, lambda_L=lambda_L, L=L, sigmaL=gammH5)\n",
    "    diff_loss5.append(loss5[-1]-loss(W5, H5))\n",
    "    W5 = multiplicative_step_w(X, G, W5, H5)\n",
    "    loss5.append(loss(W5, H5))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10*maxit):\n",
    "    H2 = multiplicative_step_hq(X, G, W2, H2, force_simplex=force_simplex, lambda_L=lambda_L, L=L, sigmaL=gammH5)\n",
    "    W2 = multiplicative_step_w(X, G, W2, H2)\n",
    "\n",
    "l_infty = loss(W2, H2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = np.arange(maxit+1)+1\n",
    "plt.figure(figsize=[15, 10])\n",
    "plt.plot(iterations, np.array(loss0)-l_infty, \"ko-\", label=\"KL surrogate - theoretical step\")\n",
    "plt.plot(iterations, np.array(loss5)-l_infty, \"ro-\", label=\"KL surrogate -  manual fixed step\")\n",
    "plt.plot(iterations, np.array(loss4)-l_infty, \"go-\", label=\"KL surrogate - adaptive step size\")\n",
    "plt.plot(iterations, np.array(loss1)-l_infty, \"kx-\", label=\"L2 surrogate - theoretical step\")\n",
    "plt.plot(iterations, np.array(loss2)-l_infty, \"rx-\", label=\"L2 surrogate - manual fixed step\")\n",
    "plt.plot(iterations, np.array(loss3)-l_infty, \"gx-\", label=\"L2 surrogate - adaptive step size\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gammH3_vec, \".-\", label=\"L2 surrogate\")\n",
    "plt.plot(gammH4_vec, \".-\", label=\"DGKL surrogate\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Evolution of the step size\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f(x, a, b, c):\n",
    "#     v = x + b\n",
    "#     return v - np.sqrt( v**2 + 4*a*c) +2*a \n",
    "# p = 1\n",
    "# x = np.arange(-2,2, 0.1)\n",
    "# a = np.random.rand(p)\n",
    "# a = 0.5\n",
    "# b = np.random.rand(p)\n",
    "# c = np.random.rand(p)\n",
    "# plt.plot(x, f(x,a,b,c))\n",
    "# plt.plot(x, np.zeros_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "espm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "a7ceffc662db6c9d514927743d8d35570797b920e33613212d4f424c0416cf91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
