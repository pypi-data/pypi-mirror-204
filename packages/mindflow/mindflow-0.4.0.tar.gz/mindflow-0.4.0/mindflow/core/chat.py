from typing import List, Union

from mindflow.db.objects.conversation import Conversation
from mindflow.db.objects.static_definition.conversation import ConversationID
from mindflow.settings import Settings
from mindflow.utils.errors import ModelError
from mindflow.utils.prompt_builders import (
    Role,
    build_prompt,
    create_message,
    prune_messages,
)
from mindflow.utils.prompts import DEFAULT_CONVERSATION_SYSTEM_PROMPT
from mindflow.utils.token import (
    estimate_tokens_from_messages,
    estimate_tokens_from_paths,
)


def run_chat(document_paths: List[str], user_query: str):
    # if the document paths contains only files that are reasonably small, then we don't need to run any kind of
    # search, we can just concatenate the files and return the result.
    settings = Settings()
    completion_model = settings.mindflow_models.query.model

    # load the conversation
    conversation = Conversation.load(ConversationID.CHAT_0.value)
    if conversation is None:
        first_message = create_message(
            Role.SYSTEM.value, DEFAULT_CONVERSATION_SYSTEM_PROMPT
        )
        conversation = Conversation(
            {"id": ConversationID.CHAT_0.value, "messages": [first_message]}
        )

    # estimate the number of tokens that will be generated by the query
    tokens, texts = estimate_tokens_from_paths(
        document_paths, user_query, completion_model, return_texts=True
    )
    if tokens > completion_model.hard_token_limit:
        raise NotImplementedError(
            f"{tokens} is too large (for now), max is {completion_model.hard_token_limit}."
        )

    ## Append new text and prune old messages to fit within the model's token limit
    conversation.messages.append(create_message(Role.USER.value, "\n".join(texts)))
    conversation.messages = prune_messages(conversation.messages, completion_model)

    response: Union[ModelError, str] = completion_model(
        build_prompt(conversation.messages, completion_model)
    )
    if isinstance(response, ModelError):
        return response.message

    # track the response
    conversation.messages.append(create_message(Role.ASSISTANT.value, response))
    conversation.total_tokens = estimate_tokens_from_messages(
        conversation.messages, completion_model
    )

    conversation.save()

    return response
