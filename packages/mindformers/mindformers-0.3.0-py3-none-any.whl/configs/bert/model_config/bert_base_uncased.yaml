model:
  model_config:
    type: BertConfig
    use_one_hot_embeddings: False
    num_labels: 1
    dropout_prob: 0.1
    batch_size: 128
    seq_length: 128 #length of input sentence
    vocab_size: 30522 #size of vocab
    hidden_size: 768 #size of text feature
    num_hidden_layers: 12 #model depth
    num_attention_heads: 12 #number of attention heads
    intermediate_size: 3072 #hidden_size*4
    hidden_act: "gelu" #activation
    post_layernorm_residual: True #select postlayernorm or prelayernorm
    hidden_dropout_prob: 0.1
    attention_probs_dropout_prob: 0.1
    max_position_embeddings: 512
    type_vocab_size: 2
    initializer_range: 0.02
    use_relative_positions: False
    use_past: False
    checkpoint_name_or_path: ""
  arch:
    type: BertForPreTraining

processor:
  return_tensors: ms
  tokenizer:
    cls_token: '[CLS]'
    do_basic_tokenize: True
    do_lower_case: True
    mask_token: '[MASK]'
    pad_token: '[PAD]'
    sep_token: '[SEP]'
    type: BertTokenizer
    unk_token: '[UNK]'
  type: BertProcessor