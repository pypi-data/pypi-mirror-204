model:
  model_config:
    type: BertConfig
    use_one_hot_embeddings: False
    num_labels: 31
    dropout_prob: 0.1
    batch_size: 24
    seq_length: 128 # length of input sentence
    vocab_size: 21128 # size of vocab
    embedding_size: 768 # size of text feature
    num_layers: 12 # model depth
    num_heads: 12 # number of attention heads
    expand_ratio: 4
    hidden_act: "gelu" # activation
    post_layernorm_residual: True # select postlayernorm or prelayernorm
    hidden_dropout_prob: 0.1
    attention_probs_dropout_prob: 0.1
    max_position_embeddings: 512
    type_vocab_size: 2
    initializer_range: 0.02
    use_relative_positions: False
    use_past: False
    compute_dtype: "float32"
    checkpoint_name_or_path: "tokcls_bert_base_chinese_cluener"
  arch:
    type: BertForTokenClassification

processor:
  type: BertProcessor
  return_tensors: ms
  tokenizer:
    type: BertTokenizer
    cls_token: '[CLS]'
    mask_token: '[MASK]'
    pad_token: '[PAD]'
    sep_token: '[SEP]'
    unk_token: '[UNK]'
    is_tokenize_char: True
    do_lower_case: False
    checkpoint_name_or_path: tokcls_bert_base_chinese