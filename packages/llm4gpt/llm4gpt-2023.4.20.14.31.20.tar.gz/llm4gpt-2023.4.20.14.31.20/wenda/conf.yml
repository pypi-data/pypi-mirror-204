logging: "True"

PORT: 17860
# WebUI 端口

PYTHON: "/opt/homebrew/Caskroom/miniforge/base/envs/py38/bin/python"
# python程序位置，可搭配一键包或是省去每次切换环境

#glm_path: "/Users/betterme/PycharmProjects/AI/chatglm"

glm_path: /Users/betterme/PycharmProjects/AI/CHAT_MODEL/chatglm-6b-int4
# glm模型位置

#glm_strategy: "cuda fp16"
glm_strategy: "cpu fp32"

# glm 模型参数  支持：
# "cpu fp32"  所有glm模型 要直接跑在cpu上都可以使用这个参数
# "cpu fp32i8" fp16原生模型 要自行量化为int8跑在cpu上可以使用这个参数
# "cpu fp32i4" fp16原生模型要 自行量化为int4跑在cpu上可以使用这个参数
# "cuda fp16"  所有glm模型 要直接跑在gpu上都可以使用这个参数
# "cuda fp16i8"  fp16原生模型 要自行量化为int8跑在gpu上可以使用这个参数
# "cuda fp16i4"  fp16原生模型 要自行量化为int4跑在gpu上可以使用这个参数

glm_lora_path: ""
# glm模型lora微调权重目录路径  为空则不加载LoRA

rwkv_path: "model/RWKV-4-Raven-7B-v7-ChnEng-20230404-ctx2048.pth"
# rwkv模型位置

rwkv_strategy: "cuda fp16"
# rwkv模型参数

rwkv_lora_path: ""
# rwkv模型lora微调权重目录路径  为空则不加载LoRA

rwkv_lora_alpha: "16"
# rwkv模型lora微调权重alpha  和训练时所用值挂钩

llm_type: "glm6b"
#  LLM模型类型:glm6b、rwkv

zsk_type: "bing"
#  知识库类型:
#  s→传统索引
#  x→基于Sentence  Transformer 的向量数据库
#  bing→cn.bing搜索，仅国内可用
#  bingxs→cn.bing学术搜索，仅国内可用
#  bingsite→cn.bing站内搜索，仅国内可用，需设置网址：
# cn.bing站内搜索网址:
# site: www.jianbiaoku.com
#  建标库
site: "www.12371.cn"
#  共产党员网

zsk_show_soucre: 0
#  知识库显示来源

zsk_folder: "zsk"
#  知识库的文件夹目录名称，若留空则为txt

embeddings_path: "model/simcse-chinese-roberta-wwm-ext"
# embeddings模型位置

vectorstore_path: "xw"
# vectorstore保存位置

chunk_size: 400
# chunk_size

chunk_count: 3
# chunk_count
