<To Do>
- Implement ways for Curve fitting using output as slopes
	- Test 'derivative' functionallity
- Add a 'draw net' feature that plots a diagram of the net in pyplot
	- Normal (line) diagram
	- Heatmap visual of weights
- Extend net feature
	- Add a hidden layer
		- Train og net, add random layer, then just train that layer
- Update saving/loading nets to be done in a folder named 'mcnets'
- Add MicroTweak to change only a few parameters at a time
- Speed up net calculation for nets with many layers
- 3D net weight arrays? (I cant see why this would be an improvement)
- Backwards propogation? (Train net from x -> y; can the weights be reversed to go y -> x?)
- Languange processing
	- Letters to slope (for a sorta 'sentiment' value)
	- Given a question, have an output for yes/no
- Add intelligent data thinning function (takes points where slope changes)
- Add test/train split that keeps ordering
- Add monte-carlo tree search as another advanced training method (might not be worth it?)
	- Like genTrain, but continues testing a few routes for more than one depth
- Do test on all acti function combinations for curve fitting test (sin(x+4) - (x^2/10) + x) xE[0,11]
- Fix training with biases included in .Calculate()


<Tested>
- Biases
	- Single vector
		- Applied to all hidden layers
			- Just created a very off center line
		- Applied to input layer
			- Kinda did something, still worse behavior
	- Bias collection
		- Hidden + final layer
			- Kinda fitted, more often than not raised or lowered too much
		- Hidden layers only
			- Could fit, had some weird jumps/behavior though
- 'SSE' for error
	- Actually did sum(abs(differences)) and reported the inverse to give higher == better for training
	- Fit not terribly, but overall R^2 is still much more superior method


V0.2.2
- Updated ATAN to the numpy function (faster now lol)
- Updated ELU to be a true piece-wise exponential and linear function (not the weird thing it was before)
- Moved the old ELU definition to "EXP"
- Added the SIG (sigmoid) activation function
- Remove outdated .Calculate() string info
- Updated __init__ info string for SIG fucntion
- Updated activation function list check for too many OR too little functions provided
- Removed vector reshaping for yHat in netMetrics -> SSreg


V0.2.1
- Added >1D handling for yData in thinData()
	- Updated description
- Added 'gamma' as decay factor in genTrain()
- NOTE: found it is better to increase batch size (essentially search depth) when training
		rather than increasing iterations (and/or decreasing gamma)
- Added 'smart' batch sizing to genTrain()
	- Default is 0 which calls a depth/batchSize from 20 to 10
	- This depth value exponentially decays from iteration 1 to ~1000
- Fixed end print statments printing with silent mode on in genTrain()


V0.2.0
- Fixed 'silent' printing in CycleTrain
- Changed Forecast legend to "validation data" for comparison/Y data
- Changed forecast vali data plot to "--" type
- Added Extend()
	> Increases a net's hidden layer sizes by given int amount
	> Useful for creating large nets to train by "building up" to a larger net 
		-> (normal method) 107.7 s for 8.434 -> 1.983e-8 error
		-> (extend method) 40.2 s for 8.318 -> 3.989e-10 error
	> Imputing zeros (median/random imputing didn't work)
- Added RELU activation type
- Removed Dig() function
- Added net property activationFunction (list of functions to use for each weight layer)
	- Updated Calculate()
	- Updated CopyNet()
	- Updated SaveNet()
	- Updated LoadNet()
	- Removed hiddenFunction input from:
	  Forecast(), netMetrics(), genTrain(), Calculate()
- Added netMetrics()
	> Returns either R^2 or R^2 and the three errors used to find it
- Added genTrain()
	> New training method using a batch of new nets to test each iteration
- Added thinData()
	> Returns:
		>> thinned x points (xThin)
		>> thinned y points	(yThin)
		>> Data point indicies (xPlot)
			>>> Useful for plotting the yThin data, particularly on top of the full yData
	> NOTE:
		>> Using xThin = [*range(len(yThin))] works great for small nets, but technically
		   doesn't train to the true data. Larger nets can train to the true indicies (x data)
		   pretty well which is good, just takes more time (see genTrain #4 pic).
- Depreceated Train() and CycleTrain()


V0.1.5.3
- Changed validationVals to account for 1D vectors (LN588)
- Changed 'default' selection in TweakWeight to 'all'
- Corrected RELU and ELU names (to ELU and ATAN)


V0.1.5
- For nets with inSize = 1, a simple number can be given (no longer required to be that stupid 1x1 numpy array thing).
- Reduced maxCycles default value to 5 for the cycle training


V0.1.4
- Operational as package
- Printing an AdvNet now also prints the # of parameters it has