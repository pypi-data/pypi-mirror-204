from __future__ import annotations

from abc import ABC, abstractmethod
from pathlib import Path
from typing import Optional, Literal, overload
from ream.data_model_helper import NumpyDataModel

import torch
from torch.utils.data import DataLoader, Dataset

from ned.candidate_ranking.helpers.dataset import MyDataset
from ned.data_models.prelude import DatasetCandidateEntities, NEDExample
from tqdm import tqdm
from nptyping import Float64, NDArray, Shape, Object


class CandidateRankingMethod(torch.nn.Module, ABC):
    EXPECTED_ARGS = []
    # arguments during normal evaluation, so the model does not calculate loss
    EXPECTED_EVAL_ARGS = []
    EVAL_BATCH_SIZE = 1000
    EVAL_ON_CPU = True

    @abstractmethod
    def forward(self, *args, **kwargs) -> ModelOutput:
        pass

    @overload
    @abstractmethod
    def generate_dataset(
        self,
        examples: list[NEDExample],
        candidates: DatasetCandidateEntities,
        num_proc: Optional[int] = None,
        for_training: bool = False,
        cache_dir: Optional[Path] = None,
        return_candidates: Literal[False] = False,
    ) -> MyDataset:
        ...

    @overload
    @abstractmethod
    def generate_dataset(
        self,
        examples: list[NEDExample],
        candidates: DatasetCandidateEntities,
        num_proc: Optional[int] = None,
        for_training: bool = False,
        cache_dir: Optional[Path] = None,
        return_candidates: Literal[True] = True,
    ) -> tuple[MyDataset, DatasetCandidateEntities]:
        ...

    @abstractmethod
    def generate_dataset(
        self,
        examples: list[NEDExample],
        candidates: DatasetCandidateEntities,
        num_proc: Optional[int] = None,
        for_training: bool = False,
        cache_dir: Optional[Path] = None,
        return_candidates: bool = False,
    ) -> MyDataset | tuple[MyDataset, DatasetCandidateEntities]:
        """Generate a dataset that the model can execute on

        Args:
            for_training: if True, the dataset is generated for training purposes, this will include dev/test sets. If the dataset is used for prediction, this must be False.
            return_candidates: if False, the candidates must not be modified (usually for_training=False)
        """
        pass

    @abstractmethod
    def get_generating_dataset_args(self) -> dict[str, str]:
        """Get arguments that are specific to the dataset generated by the model.
        Note this arguments should only affect the dataset customization part as we won't use it to generate a new caching directory"""
        pass

    def rank_examples(
        self, examples: list[NEDExample], candidates: DatasetCandidateEntities
    ) -> CandidateEntityScores:
        """Ranking candidate entities in the examples"""
        ds = self.generate_dataset(examples, candidates)
        return self.rank_datasets(examples, candidates, ds)

    def rank_datasets(
        self,
        examples: list[NEDExample],
        candidates: DatasetCandidateEntities,
        dataset: Dataset,
        verbose: bool = False,
    ) -> CandidateEntityScores:
        self.eval()
        params = next(self.parameters())
        device = params.device

        dloader = DataLoader(
            dataset,
            batch_size=self.EVAL_BATCH_SIZE,
            shuffle=False,
            pin_memory=params.is_cuda,
        )

        with torch.no_grad():
            probs = []
            for batch in tqdm(
                dloader,
                total=len(dloader),
                desc="ranking candidates in a dataset",
                disable=not verbose,
            ):
                kwargs = {}
                for arg in self.EXPECTED_EVAL_ARGS:
                    kwargs[arg] = batch[arg].to(device)
                output = self.forward(**kwargs)
                probs.append(output.probs.cpu())

            probs = torch.cat(probs)

        return CandidateEntityScores(probs.numpy())


class ModelOutput:
    __slots__ = ("loss", "probs")

    def __init__(self, loss: Optional[torch.Tensor], probs: torch.Tensor):
        self.loss = loss
        self.probs = probs


class CandidateEntityScores(NumpyDataModel):
    __slots__ = ["score"]

    # score of candidate entities, the order is matched with the order in DatasetCandidateEntity
    score: NDArray[Shape["*"], Float64]

    def __init__(self, score: NDArray[Shape["*"], Float64]):
        self.score = score


CandidateEntityScores.init()
