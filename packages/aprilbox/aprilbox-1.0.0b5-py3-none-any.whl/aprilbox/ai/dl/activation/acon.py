# coding: utf-8
"""
Function

----------------------------------
Version    : 0.0.1
Date       : 2023/4/19   15:32
----------------------------------
Author     : April
Contact    : fanglwh@outlook.com
"""


__all__ = ["AconC", "MetaAconC"]

    
import torch
from torch import nn, Tensor


class AconC(nn.Module):
    """ ACON activation (activate or not).
    According to "Activate or Not: Learning Customized Activation"
    
    :math:`AconC = (p_1 * x - p_2 * x) * \\sigmoid(\\beta * p_1 * x - p_2 * x) + p_2 * x`
    beta is generated by a small network
    
    Args:
        width: The width of the channel
    
    References:
        [1] Ma N, Zhang X, Liu M, et al. Activate or not: Learning customized activation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 8032-8042.
            SOURCE: paper_, code_
        
        .. _paper: https://arxiv.org/abs/2009.04759
        .. _code: https://github.com/nmaac/acon
    """

    def __init__(self, width: int) -> None:
        super().__init__()
        self.p1 = nn.Parameter(torch.randn(1, width, 1, 1))
        self.p2 = nn.Parameter(torch.randn(1, width, 1, 1))
        self.beta = nn.Parameter(torch.ones(1, width, 1, 1))

    def forward(self, x: Tensor) -> Tensor:
        return (self.p1 * x - self.p2 * x) * torch.sigmoid(self.beta * (self.p1 * x - self.p2 * x)) + self.p2 * x


class MetaAconC(nn.Module):
    """ ACON activation (activate or not).
    According to "Activate or Not: Learning Customized Activation"
    
    :math:`MetaAconC = (p_1 * x - p_2 * x) * \\sigmoid(\\beta * p_1 * x - p_2 * x)`
    beta is generated by a small network

    Args:
        width: The width of the channel
        r: default `16`. The ratio of hte width.
        
    References:
        [1] Ma N, Zhang X, Liu M, et al. Activate or not: Learning customized activation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 8032-8042.
            SOURCE: paper_, code_
        
        .. _paper: https://arxiv.org/abs/2009.04759
        .. _code: https://github.com/nmaac/acon
    """

    def __init__(self, width: int, r: int = 16) -> None:
        super().__init__()
        self.fc1 = nn.Conv2d(width, max(r, width // r), kernel_size=1, stride=1, bias=True)
        self.bn1 = nn.BatchNorm2d(max(r, width // r))
        self.fc2 = nn.Conv2d(max(r, width // r), width, kernel_size=1, stride=1, bias=True)
        self.bn2 = nn.BatchNorm2d(width)

        self.p1 = nn.Parameter(torch.randn(1, width, 1, 1))
        self.p2 = nn.Parameter(torch.randn(1, width, 1, 1))

    def forward(self, x: Tensor) -> Tensor:
        beta = torch.sigmoid(
            self.bn2(self.fc2(self.bn1(self.fc1(x.mean(dim=2, keepdims=True).mean(dim=3, keepdims=True))))))
        return (self.p1 * x - self.p2 * x) * torch.sigmoid(beta * (self.p1 * x - self.p2 * x)) + self.p2 * x
